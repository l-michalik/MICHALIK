# ğŸ§  RAG with Cosine Similarity & LLaMA2 (via Ollama)

This project demonstrates a minimal implementation of **Retrieval-Augmented Generation (RAG)** using:
- **Cosine Similarity** for retrieving relevant context.
- **LLaMA2**, served locally via **Ollama**, for generating concise, context-aware responses.

---

## ğŸ“š What is RAG?

**Retrieval-Augmented Generation (RAG)** is a technique that combines:
1. **Retrieval** â€“ Finding relevant documents or context based on a userâ€™s input.
2. **Generation** â€“ Passing that context to a language model to produce a grounded and relevant response.

Instead of relying solely on what the model "knows", RAG provides it with up-to-date, query-specific information at runtime.

---

## ğŸ” Document Retrieval: Cosine Similarity

We use a basic form of cosine similarity to compare a user query with a set of candidate documents (called the *corpus*). Here's how it works:

- Each text is treated as a vector of word frequencies.
- The **cosine of the angle** between two such vectors represents their similarity.
- A smaller angle (higher cosine value) means greater similarity.

> ğŸ§ª Example:
> - **User Input:** â€œI want to relax outdoorsâ€
> - **Corpus:** â€œGo hikingâ€, â€œWatch TVâ€, â€œVisit a botanical gardenâ€
> - **Selected Document:** â€œVisit a botanical gardenâ€ (most similar)

---

## ğŸ§  Response Generation with LLaMA2

Once we retrieve the most relevant document, we construct a structured prompt and pass it to **LLaMA2** running via Ollama. The model is instructed to behave like a short-answer recommendation assistant:

You are a bot that makes recommendations for activities.
You answer in very short sentences and do not include extra information.
This is the recommended activity: {relevant_document}
The user input is: {user_input}
Compile a recommendation to the user based on the recommended activity and the user input.


This format ensures:
- **Concise answers**
- **Grounded recommendations**
- **Minimal hallucination**

---

## ğŸ§ª End-to-End Flow

1. **Input** â†’ User provides a query.
2. **Retrieval** â†’ System calculates cosine similarity with each document.
3. **Selection** â†’ The most similar document is chosen.
4. **Prompting** â†’ A custom prompt is built using the user input + relevant document.
5. **Generation** â†’ The prompt is sent to LLaMA2 via Ollamaâ€™s API (`/api/generate`).
6. **Response** â†’ A concise answer is streamed and printed.

---

## ğŸ’¡ Why This Approach?

This setup is ideal if you want:
- A lightweight RAG prototype
- No dependencies on embedding models or vector databases
- Simple logic using Python and a locally hosted LLM

It is suitable for:
- Activity or product recommendation bots
- FAQ assistants
- Demos or educational tools
- Quick prototyping before scaling up

