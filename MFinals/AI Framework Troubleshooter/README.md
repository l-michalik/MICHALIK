# âœ¨ AI Framework Troubleshooter

## ğŸš© Problem

Modern developers working with fast-evolving AI/ML frameworks like LangChain, HuggingFace Transformers, FastAPI, and others often face a major pain point: finding reliable, up-to-date information when things go wrong.

Despite the abundance of available documentation and community forums, information is scattered and often outdated, incomplete, or buried deep within GitHub issues, changelogs, Stack Overflow threads, and blog posts. Developers frequently encounter situations like:

- A function or class is deprecated with no mention in the main docs.
- An error message appears without context, and Stack Overflow has conflicting answers.
- A library update silently changes behavior, breaking previously working code.
- Examples in tutorials don't reflect current best practices or latest APIs.

This leads to a frustrating, time-consuming cycle of searching, skimming, and guessing â€” especially under time pressure or in production environments.

**The core problem:** There is no single source of truth that can synthesize and contextualize scattered documentation and community knowledge for a specific developer question.

**Goal of this project:** To solve that by using Retrieval-Augmented Generation (RAG) to provide developers with relevant, context-aware, and up-to-date answers â€” grounded in actual source content from documentation, GitHub repos, and forums.

## ğŸš€ Features

### 1. ğŸ” Multi-source Semantic Retrieval
Retrieve relevant content from:
- Official documentation (e.g., LangChain, Transformers)
- GitHub files (e.g., README.md, changelogs, examples)
- Stack Overflow answers and questions  
Using vector embeddings + similarity search Weawiate.

---

### 2. ğŸ§  Context-Aware Answer Generation (RAG Core)
Inject retrieved content into an LLM prompt to:
- Provide grounded, source-aware answers
- Format responses for developers (e.g., with code blocks)
- Avoid hallucination by constraining to retrieved context

---

### 3. ğŸ” Automatic Index Refresh
Regularly update vector indexes by re-fetching source data:
- GitHub API clones or direct scraping
- Stack Overflow API based on specific tags
- Documentation auto-refresh pipelines

---

### 4. ğŸ§¾ Source Attribution & Traceability
Each answer includes:
- Snippets of the original content
- Source links for validation
- Inline citations where applicable

---

### 5. ğŸ§ª Version-Aware Question Routing (Optional)
Route queries to the correct framework version by detecting:
- Version mentions in user queries (e.g., "LangChain 0.1.6")
- Changelog-aware context inclusion


## ğŸ“‚ Folder Structure

```
ai-framework-troubleshooter/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/               # Cleaned & chunked docs (stored)
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ weaviate_client.py       # Handles cloud vector index operations
â”‚   â”œâ”€â”€ schema.py                # Weaviate class/schema definitions
â”‚   â””â”€â”€ upsert.py                # Pushes documents to Weaviate
â”‚
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ github_fetcher.py
â”‚   â”œâ”€â”€ docs_scraper.py
â”‚   â”œâ”€â”€ stackoverflow_fetcher.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â””â”€â”€ embedder.py              # Sends embeddings to Weaviate
â”‚
â”œâ”€â”€ generator/
â”‚   â”œâ”€â”€ prompt_template.py
â”‚   â”œâ”€â”€ llm_runner.py            # Calls Hugging Face LLM
â”‚   â””â”€â”€ rag_pipeline.py          # Full RAG logic using cloud index + LLM
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ api.py                   # FastAPI endpoint to serve answers
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py                # Load API keys, endpoints from env
â”‚   â””â”€â”€ logging_config.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_vector_db.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ main.py                      # CLI tool (e.g., query, embed, update)
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env
â””â”€â”€ README.md
```


## ğŸ›  Setup

This project uses [`uv`](https://github.com/astral-sh/uv) â€” a superfast Python package manager â€” for dependency management.

### 4. Set up environment variables

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_key
```

## ğŸ§ª Usage

Once your environment is set up, you can start querying the assistant:

```bash
uv run main.py query --question "Why is load_chain not working?"
uv run main.py index --file docs/langchain.md --source "LangChain Docs"
uv run main.py index-github --owner langchain-ai --repo langchain
uv run main.py index-stack --question "how to load pdfs in langchain"
```

Version-aware query routing
Deployment and hosting
Documentation and usage examples